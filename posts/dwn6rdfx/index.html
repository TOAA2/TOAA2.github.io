<!doctype html><html lang="zh-CN"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="generator" content="VuePress 2.0.0-rc.24" /><meta name="theme" content="VuePress Theme Plume 1.0.0-rc.161" /><script id="check-mac-os">document.documentElement.classList.toggle('mac', /Mac|iPhone|iPod|iPad/i.test(navigator.platform))</script><script id="check-dark-mode">;(function () {const um= localStorage.getItem('vuepress-theme-appearance') || 'auto';const sm = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;const isDark = um === 'dark' || (um !== 'light' && sm);document.documentElement.dataset.theme = isDark ? 'dark' : 'light';})();</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"物理与深度学习","image":[""],"dateModified":"2025-05-30T10:21:15.000Z","author":[]}</script><meta property="og:url" content="https://physnya.top/posts/dwn6rdfx/"><meta property="og:site_name" content="菲兹克斯喵"><meta property="og:title" content="物理与深度学习"><meta property="og:description" content="——解析本年度 Nobel Prize for Physics & Chemistry. 这是物理系办的一个讲座，主要围绕今年的 Nobel 物理学奖颁发给深度学习的两位专家这个话题展开. 我在讲座上记录了一些要点，觉得很有启发，所以放在这里作为一个存档. 介绍 人工智能（Artificial Intelligence）⊇ 机器学习（Machine L..."><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2025-05-30T10:21:15.000Z"><meta property="article:tag" content="lecture"><meta property="article:tag" content="interdisciplinarity"><meta property="article:tag" content="Physics"><meta property="article:modified_time" content="2025-05-30T10:21:15.000Z"><link rel="icon" type="image/jpg" href="images/Physics_nya.jpg"><link rel="stylesheet" type="text/css" href="https://chinese-fonts-cdn.deno.dev/packages/lxgwwenkaibright/dist/LXGWBright-Regular/result.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous"><script src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script><script src="https://umami.physnya.top/script.js" data-website-id="c412eaae-10f9-4820-99bf-a6410154a744"></script><script src="https://app.rybbit.io/api/script.js" data-site-id="1153">defer</script><link rel="prefetch" as="image" href="https://www.blogsclub.org/badge/physnya.top"><link rel="me" href="https://mastodon.social/@physnya"><meta name="fediverse:creator" content="@physnya@mastodon.social"><link rel="alternate" type="application/atom+xml" href="https://physnya.top/atom.xml" title="菲兹克斯喵 Atom Feed"><title>物理与深度学习 | 菲兹克斯喵</title><meta name="description" content="——解析本年度 Nobel Prize for Physics & Chemistry. 这是物理系办的一个讲座，主要围绕今年的 Nobel 物理学奖颁发给深度学习的两位专家这个话题展开. 我在讲座上记录了一些要点，觉得很有启发，所以放在这里作为一个存档. 介绍 人工智能（Artificial Intelligence）⊇ 机器学习（Machine L..."><link rel="preload" href="/assets/style-D3URlRNF.css" as="style"><link rel="stylesheet" href="/assets/style-D3URlRNF.css"><link rel="modulepreload" href="/assets/app-BDw5boxB.js"><link rel="modulepreload" href="/assets/index.html-GJBBkIir.js"></head><body><div id="app"><!--[--><!--[--><div class="theme-plume vp-layout" vp-container data-v-88130f74><!--[--><!--[--><!--]--><!--[--><span tabindex="-1" data-v-0bf83b7f></span><a href="#VPContent" class="vp-skip-link visually-hidden" data-v-0bf83b7f> Skip to content </a><!--]--><!----><header class="vp-nav" data-v-88130f74 data-v-802a38c9><div class="vp-navbar" vp-navbar data-v-802a38c9 data-v-6e9bba0f><div class="wrapper" data-v-6e9bba0f><div class="container" data-v-6e9bba0f><div class="title" data-v-6e9bba0f><div class="vp-navbar-title" data-v-6e9bba0f data-v-e6724548><a class="vp-link link no-icon title" href="/" data-v-e6724548><!--[--><!--[--><!--]--><!--[--><!--[--><!--[--><img class="vp-image dark logo" style="" src="/images/Physics_nya.jpg" alt data-v-e7d914a0><!--]--><!--[--><img class="vp-image light logo" style="" src="/images/Physics_nya.jpg" alt data-v-e7d914a0><!--]--><!--]--><!--]--><span data-v-e6724548>菲兹克斯喵</span><!--[--><!--]--><!--]--><!----></a></div></div><div class="content" data-v-6e9bba0f><div class="content-body" data-v-6e9bba0f><!--[--><!--]--><div class="vp-navbar-search search" data-v-6e9bba0f><div class="search-wrapper" data-v-2ccf1200><!----><div id="local-search" data-v-2ccf1200><button type="button" class="mini-search mini-search-button" aria-label="Search" data-v-2ccf1200><span class="mini-search-button-container"><span class="mini-search-search-icon vpi-mini-search" aria-label="search icon"></span><span class="mini-search-button-placeholder">Search</span></span><span class="mini-search-button-keys"><kbd class="mini-search-button-key"></kbd><kbd class="mini-search-button-key">K</kbd></span></button></div></div></div><!--[--><!--]--><nav aria-labelledby="main-nav-aria-label" class="vp-navbar-menu menu" data-v-6e9bba0f data-v-b4dab381><span id="main-nav-aria-label" class="visually-hidden" data-v-b4dab381>Main Navigation</span><!--[--><!--[--><a class="vp-link link navbar-menu-link" href="/" tabindex="0" data-v-b4dab381 data-v-bbc56676><!--[--><!----><span data-v-bbc56676>首页</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/blog/" tabindex="0" data-v-b4dab381 data-v-bbc56676><!--[--><!----><span data-v-bbc56676>博客</span><!----><!--]--><!----></a><!--]--><!--[--><div class="vp-flyout vp-navbar-menu-group" data-v-b4dab381 data-v-359eb157><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-359eb157><span class="text" data-v-359eb157><!----><!----><span data-v-359eb157>笔记</span><!----><span class="vpi-chevron-down text-icon" data-v-359eb157></span></span></button><div class="menu" data-v-359eb157><div class="vp-menu" data-v-359eb157 data-v-162d9419><div class="items" data-v-162d9419><!--[--><!--[--><div class="vp-menu-link" data-v-162d9419 data-v-df8b5cf4><a class="vp-link link" href="/Feynman-III/" data-v-df8b5cf4><!--[--><!----> Feynman III 札记 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-162d9419 data-v-df8b5cf4><a class="vp-link link" href="/integral/" data-v-df8b5cf4><!--[--><!----> 高等微积分 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-162d9419 data-v-df8b5cf4><a class="vp-link link" href="/cosmos/" data-v-df8b5cf4><!--[--><!----> 星系与宇宙 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-162d9419 data-v-df8b5cf4><a class="vp-link link" href="/writing/" data-v-df8b5cf4><!--[--><!----> 写作与沟通 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-162d9419 data-v-df8b5cf4><a class="vp-link link" href="/complex/" data-v-df8b5cf4><!--[--><!----> 复变函数 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-group" data-v-162d9419 data-v-3b3fbac5><p class="title" data-v-3b3fbac5><!----><span data-v-3b3fbac5>自学笔记</span></p><!--[--><!--[--><div class="vp-menu-link" data-v-3b3fbac5 data-v-df8b5cf4><a class="vp-link link" href="/self-learn-GR/" data-v-df8b5cf4><!--[--><!----> 广义相对论 <!----><!--]--><!----></a></div><!--]--><!--]--></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="vp-flyout vp-navbar-menu-group" data-v-b4dab381 data-v-359eb157><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-359eb157><span class="text" data-v-359eb157><!----><!----><span data-v-359eb157>关于</span><!----><span class="vpi-chevron-down text-icon" data-v-359eb157></span></span></button><div class="menu" data-v-359eb157><div class="vp-menu" data-v-359eb157 data-v-162d9419><div class="items" data-v-162d9419><!--[--><!--[--><div class="vp-menu-link" data-v-162d9419 data-v-df8b5cf4><a class="vp-link link" href="/about/" data-v-df8b5cf4><!--[--><!----> 关于我 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-162d9419 data-v-df8b5cf4><a class="vp-link link" href="/timeline/" data-v-df8b5cf4><!--[--><!----> 时间线 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-162d9419 data-v-df8b5cf4><a class="vp-link link" href="/talks/" data-v-df8b5cf4><!--[--><!----> 动态 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-162d9419 data-v-df8b5cf4><a class="vp-link link" href="/bangumi/" data-v-df8b5cf4><!--[--><!----> Bangumi <!----><!--]--><!----></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/links/" tabindex="0" data-v-b4dab381 data-v-bbc56676><!--[--><!----><span data-v-bbc56676>友链</span><!----><!--]--><!----></a><!--]--><!--]--></nav><!--[--><!--]--><!----><div class="vp-navbar-appearance appearance" data-v-6e9bba0f data-v-55eb4e61><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-55eb4e61 data-v-c7d6c3e0 data-v-609c7ff0><span class="check" data-v-609c7ff0><span class="icon" data-v-609c7ff0><!--[--><span class="vpi-sun sun" data-v-c7d6c3e0></span><span class="vpi-moon moon" data-v-c7d6c3e0></span><!--]--></span></span></button></div><div class="vp-social-links vp-navbar-social-links social-links" data-v-6e9bba0f data-v-579947b9 data-v-f063591d><!--[--><!--]--></div><div class="vp-flyout vp-navbar-extra extra" data-v-6e9bba0f data-v-e14a6536 data-v-359eb157><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-359eb157><span class="vpi-more-horizontal icon" data-v-359eb157></span></button><div class="menu" data-v-359eb157><div class="vp-menu" data-v-359eb157 data-v-162d9419><!----><!--[--><!--[--><!----><div class="group" data-v-e14a6536><div class="item appearance" data-v-e14a6536><p class="label" data-v-e14a6536>外观</p><div class="appearance-action" data-v-e14a6536><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-e14a6536 data-v-c7d6c3e0 data-v-609c7ff0><span class="check" data-v-609c7ff0><span class="icon" data-v-609c7ff0><!--[--><span class="vpi-sun sun" data-v-c7d6c3e0></span><span class="vpi-moon moon" data-v-c7d6c3e0></span><!--]--></span></span></button></div></div></div><div class="group" data-v-e14a6536><div class="item social-links" data-v-e14a6536><div class="vp-social-links social-links-list" data-v-e14a6536 data-v-f063591d><!--[--><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--[--><!--[--><div class="color-picker" data-v-72e865d3><button class="toggle-button" data-v-72e865d3>主题颜色</button><!----></div><!--]--><!--]--><!--]--><button type="button" class="vp-navbar-hamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="nav-screen" data-v-6e9bba0f data-v-98400603><span class="container" data-v-98400603><span class="top" data-v-98400603></span><span class="middle" data-v-98400603></span><span class="bottom" data-v-98400603></span></span></button></div></div></div></div><div class="divider" data-v-6e9bba0f><div class="divider-line" data-v-6e9bba0f></div></div></div><!----></header><div class="vp-local-nav fixed reached-top is-blog" data-v-88130f74 data-v-a86ba760><button class="hidden menu" disabled aria-expanded="false" aria-controls="SidebarNav" data-v-a86ba760><span class="vpi-align-left menu-icon" data-v-a86ba760></span><span class="menu-text" data-v-a86ba760>Menu</span></button><div class="vp-local-nav-outline-dropdown" style="--vp-vh:0px;" data-v-a86ba760 data-v-fca4d34f><button data-v-fca4d34f>返回顶部</button><!----></div></div><!----><!--[--><div id="VPContent" vp-content class="vp-content" data-v-88130f74 data-v-acca371d><div class="vp-doc-container is-blog" data-v-acca371d data-v-3c52d9d6><!--[--><!--]--><div class="container" data-v-3c52d9d6><!----><div class="content" data-v-3c52d9d6><div class="content-container" data-v-3c52d9d6><!--[--><!--]--><main class="main" data-v-3c52d9d6><nav class="vp-breadcrumb" data-v-3c52d9d6 data-v-a4810e1c><ol vocab="https://schema.org/" typeof="BreadcrumbList" data-v-a4810e1c><!--[--><li property="itemListElement" typeof="ListItem" data-v-a4810e1c><a class="vp-link link breadcrumb" href="/" property="item" typeof="WebPage" data-v-a4810e1c><!--[-->首页<!--]--><!----></a><span class="vpi-chevron-right" data-v-a4810e1c></span><meta property="name" content="首页" data-v-a4810e1c><meta property="position" content="1" data-v-a4810e1c></li><li property="itemListElement" typeof="ListItem" data-v-a4810e1c><a class="vp-link link breadcrumb" href="/blog/" property="item" typeof="WebPage" data-v-a4810e1c><!--[-->博客<!--]--><!----></a><span class="vpi-chevron-right" data-v-a4810e1c></span><meta property="name" content="博客" data-v-a4810e1c><meta property="position" content="2" data-v-a4810e1c></li><li property="itemListElement" typeof="ListItem" data-v-a4810e1c><a class="vp-link link breadcrumb" href="/blog/categories/?id=eed5ef" property="item" typeof="WebPage" data-v-a4810e1c><!--[-->lectures<!--]--><!----></a><span class="vpi-chevron-right" data-v-a4810e1c></span><meta property="name" content="lectures" data-v-a4810e1c><meta property="position" content="3" data-v-a4810e1c></li><li property="itemListElement" typeof="ListItem" data-v-a4810e1c><a class="vp-link link breadcrumb current" href="/posts/dwn6rdfx/" property="item" typeof="WebPage" data-v-a4810e1c><!--[-->物理与深度学习<!--]--><!----></a><!----><meta property="name" content="物理与深度学习" data-v-a4810e1c><meta property="position" content="4" data-v-a4810e1c></li><!--]--></ol></nav><!--[--><!--]--><!--[--><h1 class="vp-doc-title page-title" data-v-21262434><!----> 物理与深度学习 <!----></h1><div class="vp-doc-meta" data-v-21262434><!--[--><!--]--><p class="reading-time" data-v-21262434><span class="vpi-books icon" data-v-21262434></span><span data-v-21262434>约 1749 字</span><span data-v-21262434>大约 6 分钟</span></p><p data-v-21262434><span class="vpi-tag icon" data-v-21262434></span><!--[--><a class="vp-link link tag vp-tag-eaig" href="/blog/tags/?tag=Physics" data-v-21262434><!--[-->Physics<!--]--><!----></a><a class="vp-link link tag vp-tag-l1xl" href="/blog/tags/?tag=interdisciplinarity" data-v-21262434><!--[-->interdisciplinarity<!--]--><!----></a><a class="vp-link link tag vp-tag-ggm1" href="/blog/tags/?tag=lecture" data-v-21262434><!--[-->lecture<!--]--><!----></a><!--]--></p><!--[--><!--]--><p class="create-time" data-v-21262434><span class="vpi-clock icon" data-v-21262434></span><span data-v-21262434>2024-10-21</span></p></div><!--]--><!--[--><!--]--><div class="_posts_dwn6rdfx_ external-link-icon-enabled vp-doc plume-content" vp-content data-v-3c52d9d6><!--[--><!--]--><div data-v-3c52d9d6><p>——解析本年度 Nobel Prize for Physics &amp; Chemistry.</p><p>这是物理系办的一个讲座，主要围绕今年的 Nobel 物理学奖颁发给深度学习的两位专家这个话题展开. 我在讲座上记录了一些要点，觉得很有启发，所以放在这里作为一个存档.</p><h2 id="介绍" tabindex="-1"><a class="header-anchor" href="#介绍"><span>介绍</span></a></h2><p>人工智能（Artificial Intelligence）<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊇</mo></mrow><annotation encoding="application/x-tex">\supseteq</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7719em;vertical-align:-0.136em;"></span><span class="mrel">⊇</span></span></span></span> 机器学习（Machine Learning）<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊇</mo></mrow><annotation encoding="application/x-tex">\supseteq</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7719em;vertical-align:-0.136em;"></span><span class="mrel">⊇</span></span></span></span> 深度学习（Deep Learning），这是一个包含的关系. 深度学习是这个领域的前沿，用模拟人类神经网络的方式实现人工智能.</p><p>最早在 1940 年代，提出数学模型；1958 年 Rosenblatt 感知器提出，是有隐藏层的前馈网络；日本科学家也在早期做出了很多奠基性工作.</p><p>1970 年代，这个领域进入一个寒冬，因为难以处理数量巨大的神经元. 解决这个问题的正是 John Hopfield ，今年的 Nobel Prize for Physics 得主. 他 1933 年出生，1969 得到巴克利奖（美国的一个凝聚态物理的奖项），之后想转到生物物理，但是后来发展出人工神经网络.</p><p>学过凝聚态物理的同学会知道，很多集体效应不是单个粒子的简单相加，而是呈现出非常特殊的统计性质. Hopfield 将这些思想应用到生物物理的领域中，他研究 DNA 的复制的试错，之后转向对联想记忆问题的研究.</p><blockquote><p>联想记忆：从不完整或者有偏差的输入，还原记忆内容；这是区别于传统的计算机存储的.</p></blockquote><p>Hopfield 考虑构建一个数学上的简单模型：Hopfield 网络，类似于 Ising Model ，神经元状态表征为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">s_i=0,1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em;"></span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span></span></span></span>，链接权重为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msub><mi>w</mi><mrow><mi>j</mi><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{ij}=w_{ji}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ji</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>，引入能量函数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo>=</mo><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msub><mo>∑</mo><mrow><mi>i</mi><mo mathvariant="normal">≠</mo><mi>j</mi></mrow></msub><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>s</mi><mi>i</mi></msub><msub><mi>s</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">E=-\frac{1}{2}\sum_{i\neq j}w_{ij}s_is_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2809em;vertical-align:-0.4358em;"></span><span class="mord">−</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1864em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight"><span class="mrel mtight"><span class="mord vbox mtight"><span class="thinbox mtight"><span class="rlap mtight"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="inner"><span class="mord mtight"><span class="mrel mtight"></span></span></span><span class="fix"></span></span></span></span></span><span class="mrel mtight">=</span></span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4358em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>（这和 Ising Model 简直如出一辙！）</p><p>对这个网络做两件事情：</p><ol><li>训练：优化权重<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>，用能量极小值的点<u>记录“记忆”</u>. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span> 赫布定律（1949）</li><li>预测：固定<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>，从输入的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">s_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>出发，优化 Hopfield 能量，<u>寻找相似的记忆</u>. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span> 能量极小值对应“吸引子”，吸引子状态编码信息.</li></ol><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⟹</mo></mrow><annotation encoding="application/x-tex">\Longrightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.549em;vertical-align:-0.024em;"></span><span class="mrel">⟹</span></span></span></span> 借助能量函数存储记忆，通过优化能量函数还原记忆.</p><p>这种方式被应用在早期的一些算法问题上，比如“行商问题”. 经过长期发展，现在的“现代 Hopfield 网络”已经发展到新的样式，技术内核已经更新，但是设计理念没有太大变化.</p><p>接下来， Hinton 等人对 Boltzmann 机的研究将这些知识向前更进一步：这里关注能量函数的<u>概率分布</u>，用 Monte Carlo 方法计算结果. Boltzmann 机除了权重<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>以外还引入偏置<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\theta_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，将能量极小值的记忆存储改为马尔科夫链蒙特卡洛算法（MCMC）产生的概率分布存储记忆，依靠新的方法进行训练.</p><p>Hinton 发展受限 Boltzmann 机，同层神经元之间无连接，使得理论得到大幅深入. 2017 年，这种方法被用来求解量子多体问题.</p><p>Hinton 被称为“AI 教父”，坚持挺过了 AI 研究的两度寒冬，不仅自己做出很多贡献，同时还培养了一批知名的学生，其中甚至有 OpenAI 的主要技术贡献者. 他们发现，当神经网络的 Scale 一旦变大，很多之前无法解决的问题都能迎刃而解.</p><p>之后的 AI 发展从两个角度出发：架构更先进、应用更广泛，前者的著名例子是卷积网络（CNN）的图像处理，而后者最著名的则是 ChatGPT.</p><p>Nobel 委员会在介绍此次颁奖的成果时，提到这项成果对于天体物理（黑洞计算）和 Alpha-Fold 等各种领域都实现了广泛的应用.</p><h2 id="和我们" tabindex="-1"><a class="header-anchor" href="#和我们"><span>和我们？</span></a></h2><p>徐老师谈谈 AI 对他自己专业的帮助：从基于经验到基于数据. 人工智能非常适合处理这样的问题：高维的参数空间、明确的优化目标、丰富的训练数据或高效的数据产生器，当满足这些条件时，AI 就变得非常强大，Alpha-Fold 就是一个绝佳的例子. 传统的科学计算在数值模拟耗时非常大，而有深度学习驱动的科学计算可以实现高效而智能的科学计算.</p><p>徐老师做的是第一性原理计算，即“基于量子力学原理，计算现实模型”. 这几年发展出高效的深度学习新一代第一性原理计算方法，将重要的物理先验融入神经网络，“压缩” DFT 底层算法，（局域性原理：小体系演化至大体系，速度快；协变性原理：进一步增强泛化能力，举一反三，可扩展性强）. 这是一个全新的材料研究方式：MIND（Material INtelligence Database）.</p><p>推荐一些开源的学习资料：<a href="https://github.com/mzjb/">mzjb</a>（是徐老师的学生）有很多 repo ，如徐老师和学生共同做的 DeepH .</p><h2 id="提问" tabindex="-1"><a class="header-anchor" href="#提问"><span>提问</span></a></h2><ul><li>神经网络发现物理规律的可解释性？</li></ul><p>让神经网络本身符合物理的特性，比如做反对称波函数的时候，整个神经网络本身就是反对称的；但是这还是一个非常重要而且专业的问题，也是学界关注的重点.</p><ul><li>其它学科的学生如何入门？</li></ul><p>学过四大力学，学 AI 就没有压力！之后， AI 可能会像英语一样成为一个普适的工具，反而更需要交叉学科的人才涌入. 未来会有很多可能性.</p><ul><li>AI for Physics &amp; Physics for AI？</li></ul><p>现在的 AI 像炼丹或者玄学，我们之后的工作或许是理解如何设计模型，才能让 AI 的发展更加健康；但是目前人们的目光主要聚焦在如何用好 AI 的方面.</p><ul><li>AI 的研究需要大量数据，如何收集或者产生？</li></ul><p>AI for Science 的应用中，有时需要数据产生器，或者通过经验优化模型，因为我们科学实验的数据是非常难以产生的.</p><ul><li>能否用 AI 设计 AI 算法？</li></ul><p>如果某一个 AI 专门被训练做这件事情，比如一个通用人工智能，很有可能实现.</p><h2 id="主持人的一些小分享" tabindex="-1"><a class="header-anchor" href="#主持人的一些小分享"><span>主持人的一些小分享</span></a></h2><p>其实 Hopfield 在 Ph.D 时期发的论文已经对凝聚态物理做出了很大的贡献，甚至有很多凝聚态的开创性工作，是一个非常纯正的 Theorist.</p></div><!----><!----><div class="vp-doc-copyright" data-v-3c52d9d6><h2 id="doc-copyright" tabindex="-1" class="vp-doc-header" data-v-01e9e36c><a href="#doc-copyright" class="header-anchor" data-v-01e9e36c><span data-v-01e9e36c><!--[-->版权所有<!--]--></span></a></h2><div class="hint-container tip copyright-container" data-v-91a3dbd7><p data-v-91a3dbd7><span data-v-91a3dbd7>版权归属：</span><a class="vp-link link no-icon" href="https://github.com/physnya" target="_blank" rel="noreferrer" data-v-91a3dbd7><!--[-->physnya<!--]--><!----></a></p><!----><p data-v-91a3dbd7><span data-v-91a3dbd7>许可证：</span><a class="vp-link link no-icon" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noreferrer" data-v-91a3dbd7><!--[-->署名-非商业性-相同方式共享 4.0 国际 (CC-BY-NC-SA-4.0)<!--]--><!----></a><!--[--><span class="vpi-license-cc" data-v-91a3dbd7></span><span class="vpi-license-by" data-v-91a3dbd7></span><span class="vpi-license-nc" data-v-91a3dbd7></span><span class="vpi-license-sa" data-v-91a3dbd7></span><!--]--></p></div></div></div></main><footer class="vp-doc-footer" data-v-3c52d9d6 data-v-c166b588><!--[--><!--]--><!----><div class="contributors" aria-label="Contributors" data-v-c166b588><span class="contributors-label" data-v-c166b588>贡献者: </span><span class="contributors-info" data-v-c166b588><!--[--><!--[--><span class="contributor" data-v-c166b588>physnya</span><!----><!--]--><!--]--></span></div><nav class="prev-next" data-v-c166b588><div class="pager" data-v-c166b588><a class="vp-link link pager-link prev" href="/posts/u9uwmglr/" data-v-c166b588><!--[--><span class="desc" data-v-c166b588>上一页</span><span class="title" data-v-c166b588>流水账 Week 7</span><!--]--><!----></a></div><div class="pager" data-v-c166b588><a class="vp-link link pager-link next" href="/posts/mjf6caoa/" data-v-c166b588><!--[--><span class="desc" data-v-c166b588>下一页</span><span class="title" data-v-c166b588>流水账 Week 6</span><!--]--><!----></a></div></nav></footer><div id="comment" class="twikoo-wrapper vp-comment" vp-comment darkmode="false" style="display:block;" data-v-3c52d9d6><div style="display: flex;align-items: center;justify-content: center;height: 96px"><span style="--loading-icon: url(&quot;data:image/svg+xml;utf8,%3Csvg xmlns=&#39;http://www.w3.org/2000/svg&#39; preserveAspectRatio=&#39;xMidYMid&#39; viewBox=&#39;25 25 50 50&#39;%3E%3CanimateTransform attributeName=&#39;transform&#39; type=&#39;rotate&#39; dur=&#39;2s&#39; keyTimes=&#39;0;1&#39; repeatCount=&#39;indefinite&#39; values=&#39;0;360&#39;%3E%3C/animateTransform%3E%3Ccircle cx=&#39;50&#39; cy=&#39;50&#39; r=&#39;20&#39; fill=&#39;none&#39; stroke=&#39;currentColor&#39; stroke-width=&#39;4&#39; stroke-linecap=&#39;round&#39;%3E%3Canimate attributeName=&#39;stroke-dasharray&#39; dur=&#39;1.5s&#39; keyTimes=&#39;0;0.5;1&#39; repeatCount=&#39;indefinite&#39; values=&#39;1,200;90,200;1,200&#39;%3E%3C/animate%3E%3Canimate attributeName=&#39;stroke-dashoffset&#39; dur=&#39;1.5s&#39; keyTimes=&#39;0;0.5;1&#39; repeatCount=&#39;indefinite&#39; values=&#39;0;-35px;-125px&#39;%3E%3C/animate%3E%3C/circle%3E%3C/svg%3E&quot;);--icon-size: 48px;display: inline-block;width: var(--icon-size);height: var(--icon-size);background-color: currentcolor;-webkit-mask-image: var(--loading-icon);mask-image: var(--loading-icon)"></span></div><div id="twikoo-comment"></div></div><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!--]--><button style="display:none;" type="button" class="vp-back-to-top" aria-label="back to top" data-v-88130f74 data-v-c879ce55><span class="percent" data-allow-mismatch data-v-c879ce55>0%</span><span class="show icon vpi-back-to-top" data-v-c879ce55></span><svg aria-hidden="true" data-v-c879ce55><circle cx="50%" cy="50%" data-allow-mismatch style="stroke-dasharray:calc(0% - 12.566370614359172px) calc(314.1592653589793% - 12.566370614359172px);" data-v-c879ce55></circle></svg></button><footer class="vp-footer" vp-footer data-v-88130f74 data-v-ebb1e55a><!--[--><div class="container" data-v-ebb1e55a><p class="message" data-v-ebb1e55a>Powered by <a target="_blank" href="https://v2.vuepress.vuejs.org/">VuePress</a> & <a target="_blank" href="https://theme-plume.vuejs.press">vuepress-theme-plume</a></p><p class="copyright" data-v-ebb1e55a>Copyright © 2024 - present by physnya</p></div><!--]--></footer><!--[--><!--]--><!--]--></div><!----><!--]--><!--[--><!--]--><!--]--></div><script type="module" src="/assets/app-BDw5boxB.js" defer></script></body></html>